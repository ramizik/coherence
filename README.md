# ğŸ¯ Coherence - AI Presentation Coach

**Win your next presentation with AI-powered body language feedback.**

Coherence is the first AI platform that detects **visual-verbal dissonance** â€” when your body language contradicts what you're saying. Built for students, professionals, and anyone who wants to present with confidence.

---

## ğŸš€ The Problem

- **75% of people** fear public speaking more than death
- **90% of presentation anxiety** stems from lack of objective feedback
- Existing tools (Yoodli, PowerPoint Coach) only analyze audio
- **55% of communication is non-verbal**, yet no tool catches body language mistakes

### What We Catch

âŒ **Emotional Mismatch** - Saying "I'm thrilled" with an anxious face
âŒ **Missing Gestures** - Saying "look at this chart" without pointing
âŒ **Pacing Issues** - Showing dense slides too briefly for comprehension

---

## ğŸ¥ Demo Video

> [Insert 2-minute demo video link here]

**Local Setup:** Run frontend and backend locally for development and testing

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js 14    â”‚  â† Frontend (TypeScript + TailwindCSS)
â”‚  Local Dev      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FastAPI      â”‚  â† Backend (Python)
â”‚  Local Server   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼    â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Twelve â”‚ â”‚ Deepgram â”‚ â”‚ Gemini  â”‚
â”‚ Labs  â”‚ â”‚  (Audio) â”‚ â”‚ (Brain) â”‚
â”‚(Video)â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Frontend**
- Next.js 14 (App Router) - React framework
- TypeScript - Type safety
- TailwindCSS - Glassmorphic UI
- Lucide React - Icon system

**Backend**
- FastAPI - Python async web framework
- FFmpeg - Video frame extraction
- Pydantic - Data validation

**AI Services**
- **TwelveLabs** - Semantic video search (body language analysis)
- **Deepgram** - Real-time speech transcription
- **Gemini 1.5 Pro** - Multimodal synthesis & dissonance detection

---

## ğŸ¯ Key Features

### 1. Visual-Verbal Dissonance Detection
Our unique AI analyzes three dimensions simultaneously:
- **Speech Content** (what you say)
- **Body Language** (how you look)
- **Slide Pacing** (what you show)

### 2. Actionable Coaching
Not just metrics â€” we tell you exactly how to improve:
> "You said 'passionate' at 2:15 but your face showed anxiety. **Fix:** Smile with teeth and lean forward 10Â° when expressing enthusiasm."

### 3. Interactive Timeline
Color-coded heatmap showing exactly when dissonance occurs. Click any moment to jump to that timestamp.

### 4. Coherence Score (0-100)
Weighted algorithm combining:
- Eye contact percentage (30%)
- Filler word count (25%)
- Fidgeting frequency (20%)
- Speaking pace (15%)
- Dissonance penalties (10%)

---

## ğŸ“Š Sample Analysis

**Input:** 3-minute MBA pitch video

**Output:**
```json
{
  "coherenceScore": 67,
  "metrics": {
    "eyeContact": 85,
    "fillerWords": 8,
    "fidgeting": 6,
    "speakingPace": 142
  },
  "dissonanceFlags": [
    {
      "timestamp": 15.5,
      "type": "EMOTIONAL_MISMATCH",
      "severity": "HIGH",
      "description": "Said 'thrilled' with anxious expression",
      "coaching": "Smile with teeth, lean forward 10Â°"
    }
  ]
}
```

---

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- Python 3.10+
- FFmpeg installed
- API keys for TwelveLabs, Deepgram, Gemini

### Frontend Setup
```bash
cd frontend
npm install
cp .env.example .env.local
# Add API endpoint URL to .env.local
npm run dev
# Opens at http://localhost:3000
```

### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Add API keys to .env
uvicorn app.main:app --reload
# Runs at http://localhost:8000
```

### Environment Variables

**Frontend** (`.env.local`)
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

**Backend** (`.env`)
```
TWELVELABS_API_KEY=your_key_here
DEEPGRAM_API_KEY=your_key_here
GEMINI_API_KEY=your_key_here
```

---

## ğŸ“ Project Structure

```
coherence/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ upload/page.tsx
â”‚   â”‚   â”œâ”€â”€ processing/[id]/page.tsx
â”‚   â”‚   â””â”€â”€ results/[id]/page.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ dashboard/
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ mock-data.ts
â”‚       â””â”€â”€ services/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CLAUDE.md           # AI assistant guidelines
â”‚   â”œâ”€â”€ FIGMA_MAKE.md       # Frontend generation spec
â”‚   â””â”€â”€ API.md              # API documentation
â””â”€â”€ README.md
```

---

## ğŸª Demo Preparation

### Pre-Demo Checklist
- [ ] Index 3 sample videos in TwelveLabs
- [ ] Cache analysis results for instant loading
- [ ] Test offline mode (cached fallback)
- [ ] Verify all API keys are active
- [ ] Rehearse 3-minute pitch 4+ times
- [ ] Test local file upload works smoothly
- [ ] Backup laptop with identical local setup

### Demo Flow (3 minutes)
1. **[0:00-0:30]** Hook - Explain dissonance problem
2. **[0:30-1:30]** Show pre-analyzed sample with red flags
3. **[1:30-2:30]** Live demo - upload & analyze local video
4. **[2:30-3:00]** Close - Market size & CTA

---

## ğŸ§ª Testing

### Run Frontend Tests
```bash
cd frontend
npm test
```

### Run Backend Tests
```bash
cd backend
pytest tests/ -v
```

### Test Coverage Focus
- Upload â†’ Processing â†’ Results flow (critical path)
- API endpoint response shapes
- Dissonance detection logic
- Coherence score calculation

---

## ğŸ¯ Target Users

1. **College Students** (Primary)
   - Final presentations cause 95% anxiety
   - Need objective feedback before high-stakes demos
   - Budget-conscious ($9/month tier)

2. **MBA Students** (Secondary)
   - Pitch competitions with real money at stake
   - Want competitive edge in delivery
   - Willing to pay for premium features

3. **Corporate Sales** (Tertiary)
   - Training teams for client demos
   - B2B sales where delivery = deal closure
   - Enterprise contracts ($99/seat)

---

## ğŸ’° Business Model (Post-Hackathon)

### Freemium SaaS
- **Free Tier:** 1 video/month, basic metrics
- **Student ($9/mo):** 10 videos/month, full coaching
- **Professional ($29/mo):** Unlimited videos, slide analysis
- **Enterprise ($99/seat):** Team analytics, integrations

### Market Opportunity
- **TAM:** 50M students + professionals presenting annually
- **SAM:** 10M active presentation tool users
- **SOM:** 500K early adopters (Year 1 target)

---

## ğŸ“š Documentation

- [Frontend Guidelines](documentation/FIGMA_GUIDELINES.md)
- [Claude Guidelines](CLAUDE.md)

---

## ğŸ› Known Limitations (Hackathon Scope)

- âŒ No user authentication
- âŒ No video editing/trimming
- âŒ No database persistence (in-memory cache only)
- âŒ No mobile app (web-only, local-only)
- âŒ No real-time streaming analysis
- âŒ Processing limited to 3-minute videos

---

## ğŸ“„ License

MIT License - Built for SB Hacks 2025

---

## ğŸ™ Acknowledgments

- **TwelveLabs** - Semantic video understanding API
- **Deepgram** - Real-time speech transcription
- **Google** - Gemini 1.5 Pro multimodal AI
- **Next.js** - Frontend framework
- **FastAPI** - Backend framework

---

**Built with â¤ï¸ in 24 hours | SBHacks 2025**